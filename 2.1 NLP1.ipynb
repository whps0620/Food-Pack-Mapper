{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all = pd.read_csv('goldenTRANSMAT-all.csv')\n",
    "a1 = pd.read_csv('goldenTRANSMAT-annotator1.csv')\n",
    "a2 = pd.read_csv('goldenTRANSMAT-annotator2.csv')\n",
    "a3 = pd.read_csv('goldenTRANSMAT-annotator3.csv')\n",
    "df = pd.concat([a1,a2,a3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df['Doc'].unique()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def extract_keywords_tfidf(text):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "    keywords = [(feature_names[i], tfidf_scores[i]) for i in range(len(feature_names))]\n",
    "    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "    return keywords\n",
    "\n",
    "from yake import KeywordExtractor\n",
    "def extract_keywords_rake(text):\n",
    "    extractor = KeywordExtractor()\n",
    "    keywords = extractor.extract_keywords(text)\n",
    "    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = s\n",
    "\n",
    "tfidf_keywords = extract_keywords_tfidf(text)\n",
    "rake_keywords = extract_keywords_rake(text)\n",
    "\n",
    "print(\"TF-IDF keywords:\", tfidf_keywords)\n",
    "print(\"RAKE keywords:\", rake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dependency_parsing(text):\n",
    "    # Parse the input text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract and display the dependency relations\n",
    "    dependencies = []\n",
    "    for token in doc:\n",
    "        dependencies.append((token.text, token.dep_, token.head.text))\n",
    "    \n",
    "    return dependencies\n",
    "\n",
    "# Example usage\n",
    "text = \"Barrier properties of nylon 6-montmorillonite nanocomposite membranes prepared by melt blending.\"\n",
    "parsed_dependencies = dependency_parsing(text)\n",
    "\n",
    "# Print the results\n",
    "for word, dep, head in parsed_dependencies:\n",
    "    print(f\"Word: {word}, Dependency: {dep}, Head: {head}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define the text\n",
    "text = \"Barrier and surface properties of chitosan-coated greaseproof paper.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize variables to store extracted components\n",
    "base_material = None\n",
    "treatment_type = None\n",
    "secondary_material = None\n",
    "\n",
    "# Iterate through the parsed tokens to identify the components\n",
    "for token in doc:\n",
    "    # Identify base material (the last noun in the last prepositional phrase)\n",
    "    if token.dep_ == 'pobj' and token.head.text == 'of':\n",
    "        base_material = token.text  # Base material is the object of the preposition \"of\"\n",
    "    \n",
    "    # Identify secondary material (look for the first part of the hyphenated compound)\n",
    "    if token.dep_ == 'compound' and token.head.dep_ == 'pobj':\n",
    "        secondary_material = token.text  # Secondary material from \"chitosan-coated\"\n",
    "    \n",
    "    # Identify treatment type (extract 'coated' from the hyphenated modifier)\n",
    "    if token.text == 'coated':\n",
    "        treatment_type = token.text\n",
    "\n",
    "# Display the results\n",
    "print(\"Extracted Components:\")\n",
    "print(f\"Base Material: {base_material}\")\n",
    "print(f\"Treatment Type: {treatment_type}\")\n",
    "print(f\"Secondary Material: {secondary_material}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def extract_keywords_tfidf(text):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    tfidf_scores = tfidf_matrix.toarray().flatten()\n",
    "    keywords = [(feature_names[i], tfidf_scores[i]) for i in range(len(feature_names))]\n",
    "    keywords = sorted(keywords, key=lambda x: x[1], reverse=True)\n",
    "    return keywords\n",
    "\n",
    "from yake import KeywordExtractor\n",
    "def extract_keywords_rake(text):\n",
    "    extractor = KeywordExtractor()\n",
    "    keywords = extractor.extract_keywords(text)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_poly['Doc'][150]\n",
    "\n",
    "tfidf_keywords = extract_keywords_tfidf(text)\n",
    "rake_keywords = extract_keywords_rake(text)\n",
    "\n",
    "print(\"TF-IDF keywords:\", tfidf_keywords)\n",
    "print(\"RAKE keywords:\", rake_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword extraction\n",
    "from yake import KeywordExtractor\n",
    "\n",
    "text = df_poly['Doc'][0]\n",
    "extractor = KeywordExtractor()\n",
    "keywords = extractor.extract_keywords(text)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "# Assume df1, df2, df3 are your three datasets\n",
    "target_counts_df1 = a1['Target'].value_counts()\n",
    "target_counts_df2 = a2['Target'].value_counts()\n",
    "target_counts_df3 = a3['Target'].value_counts()\n",
    "target_counts_df = all['Target'].value_counts()\n",
    "\n",
    "# Combine the counts into a single DataFrame\n",
    "combined_counts = pd.DataFrame({\n",
    "    'Dataset 1': target_counts_df1,\n",
    "    'Dataset 2': target_counts_df2,\n",
    "    'Dataset 3': target_counts_df3,\n",
    "    'Dataset' : target_counts_df\n",
    "}).fillna(0)  # Fill NaN values with 0 for missing values\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(combined_counts, annot=True, fmt = 'g', cmap=\"YlGnBu\")\n",
    "\n",
    "plt.title('Comparison of Target Column Value Counts Across Datasets')\n",
    "plt.ylabel('Target Values')\n",
    "plt.xlabel('Datasets')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
